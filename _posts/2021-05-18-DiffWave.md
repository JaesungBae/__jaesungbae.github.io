---
layout: single
title:  "[PR] DiffWave: A Versatile Diffusion Model For Audio Synthesis"
date:   2021-05-18 21:14:06 +0900
categories: paper-review icassp2021 tts:model
use_math: true
toc: true
---

*Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, Bryan Catanzaro* \
Computer Science and Engineering, UCSD & NVIDIA & Baidu Research \
Arxiv: [https://arxiv.org/pdf/2009.09761.pdf](https://arxiv.org/pdf/2009.09761.pdf)

## Abstract
* DiffWave: versatile (다재다능한) diffusion probabilistic model for conditional and unconditional waveform generation.
    * 1) non-autoregressive
    * 2) converts the white noise signal into structured waveform 
        * through a Markov chain 
        * with a constant number of steps at synthesis.
    * It is efficiently trained by optimizing a variant of variational bound on the data likelihood.
    * generates high-fidelity audio
        * neural vocoding conditioned on mel spectrogram
        * class-conditional generation
        * **unconditional generation**
    * 특히, challenging한 unconditional generation task 에서 autoregressive와 GAN-based waveform 모델보다 성능이 뛰어남
        * Audio quality & sample diversity

## 1. Introduction
* 이전의 waveform generation 모델
    * Likelihood-based models
        * autoregressive & flow-based models
        * audio synthesis에서 지배적인 모델
        * $\because$ simple training objective & superior ability of modeling the fine details of waveform in real data
    * Training을 위해 auxiliary loss를 필요로 하는 모델
        * flow-based models trained by distillation
        * variational auto-encoder (VAE) based model
        * generative adversarial network (GAN) based models
    * **<span style="color:red">한계:</span>** 이전의 waveform generation 모델은 강력한 local condition (mel or linguistic features)을 가지는 케이스에 집중하고 있음.
        * **<span style="color:red">한계:</span>** Autoregressive model은 unconditional한 setting에서 made-up word-like sounds 또는 inferior (열등한) samples을 만든다고 알려져 있음
        * $\because$ Long sequence (16000 samples per sec)을 conditional 한 정보 없이 생성해야 하기 때문

<br>

* Diffusion probabilistic models (= diffusion model)
    * Markov chain을 이용해서 simple distribution (e.g.,  isotropic Gaussian)을 복잡한 distribution으로 바꿔주는 generative model
    * Efficiently trained by optimizing the variational lower bound (ELBO). 
    * can use a diffusion (noise-adding) process without learnable parameters to obtain the “whitened” latents from training data.
        * Therefore, training 과정에서 추가적인 neural network 필요 없음. 
            * 다른 모델과는 반대되는 특성 (e.g., the encoder in VAE or the discriminator in GAN)
        * **<span style="color:blue">장점:</span>** 두가지 network를 joint training 함으로써 발생하는 challenging한 “posterior collapse” or “mode collapse”같은 문제가 발생하지 않음.

<br>

* DiffWave
    * Raw audio synthesis
    * Advantages
        1. non-autoregressive -> synthesis high dim. waveform in parallel
        2. flexible
            * flow-based 모델과 달리 어떠한 architectural 제한도 두지 않음.
            * flow-based model은 latent와 data 간에 bijection (일대일 대응) 해야한다는 제한이 있음. 
            * small-footprint neural network가 되도록 함 -> 그래도 성능이 좋긴함.
        3. single ELBO-based training objective w/o any auxiliary losses (spectrogram reconstruction)
        4. versatile model: condition & uncondition case 모두에서 좋은 성능의 audio 생성

<br>

* Contribution
    1. DiffWave는 feed-forward & bidirectional dilated conv. architecture를 가짐. 
        * WaveNet으로부터motivated
        * WaveNet 수준의 speech quality
        * WaveNet보다 훨씬 빠른 생성 속도
            * few sequential steps (6) 만이 필요로 되기 때문에
    2. Small DiffWave 모델은 2.64M parameters를 가지고, synthesizes 22.05 kHz high-fidelity speech (MOS: 4.37) more than 5× faster than real-time on a V100 GPU without engineered kernels
        * SOTA flow-based model 보다는 느린 속도, but 더 작은 footprint
    3. Unconditional & class-conditional waveform generation tasks에서 WaveGAN과 WaveNet에 비해 월등히 좋은 성능 얻어냄

## 2. DiffWave Architecture
* Based on a bidirectional dilated convolution architecture 
    * Different from WaveNet because there is no autoregressive
    * Similar architecture: source separation (Rethage et al., 2018; Lluís et al., 2018). 
* $N$ residual layers and $C$ residual channels.
* These layers are grouped into m blocks and each block has $n = N/m$ layers. 
* The dilation is doubled at each layer within each block, i.e., $[1, 2, 4, · · · , 2^{(n−1)}]$. 

### 2.1. Diffusion-step embedding
### 2.2. Conditional generation
* ***Local conditioner***
    * 1) Upsample the mel spectrogram to the same length as waveform through transposed 2-D convolutions. (Hifi GAN??)
    * 2) After a layer-specific Conv1×1 mapping its mel-band into 2C channels, the conditioner is added as a bias term for the dilated convolution in each residual layer.
* ***Global conditioner***
    * e.g., speaker IDs or word IDs
    * We use shared embeddings with dimension d_label = 128 in all experiments. 
    * In each residual layer, we apply a layer-specific Conv1×1 to map d_label to 2C channels
    * Add the embedding as a bias term after the dilated convolution in each residual layer. 

## 3. Experiments

### 3.1. Neural Vocoding
* ***Data***
    * LJSpeech 22.05kHz
* ***Models***
    * 30 residual layers, kernel size 3, and dilation cycle [1, 2, · · · , 512]
    * diffusion steps $T \in {20, 40, 50, 200}$
    * residual channels $C \in {64, 128}$ .
    * We use linear spaced schedule for $\beta_t \in [1 × 10−4 , 0.02]$ for DiffWave with $T=200$, and $\beta_t ∈ [1 × 10−4 , 0.05]$ for DiffWave with $T \le 50$.
    * The reason to increase $\beta_t$ for smaller $T$ is to make $q(x_T \mid x_0)$ close to platent. 
    * DiffWave (Fast)와 DiffWave는 동일하게 training 된 모델임.
* ***Conditioner***
    * 80-band mel spectrogram
    * We upsample the mel spectrogram 256 times by applying two layers of transposed 2-D convolution (in time and frequency) interleaved  with leaky ReLU $(\alpha=0.4)$
    * For each layer, the upsampling stride in time is 16 and 2-D filter sizes are [32, 3]
    * After upsampling, we use a layer-specific Conv1×1 to map the 80 mel bands into 2× residual channels
    * Then add the conditioner as a bias term for the dilated convolution before the gated-tanh nonlinearities in each residual layer.
* ***Training***
    * 16,000 samples 씩 끊어서 training
    * batch size 16, 1M steps
* ***Results***
    * DiffWave BASE (T = 40) in FP32 is 1.1× faster than real-time
    * DiffWave BASE (Fast) and DiffWave LARGE (Fast) can be 5.6× and 3.5× faster than real-time

### 3.2. Unconditional Generation
* ***Data***
    * Speech Command Dataset
        * Contains many spoken words by thousands of speakers 
        * Under various recording conditions including some very noisy environment.
        * 그중 숫자 부분만 골라서 (0~9) SC09라고 부름.
            * 31,158 training utterances (∼8.7 hours in total) by 2,032 speakers
            * 1초로 모두 동일한 길이 가짐. 16kHz.
            * Various variations (e.g., contents, speakers, speech rate, recording conditions)을 포함한 dataset임.

* ***Models***
    * Parallel WaveGAN도 시도해봤으나, Gan-based vocodoer는 unconditional한 task에서 생성하지 못하는 것으로 판단.
    * DiffWAve
        * 36-layer DiffWave model with kernel size 3 and dilation cycle [1, 2, · · · , 2048].
        * Diffusion steps T = 200 and residual channels $C = 256$.
        * Linear spaced schedule for $\beta_t \in [1 × 10−4 , 0.02]$ .

* ***Training***
    * fix the learning rate to $2 * 10^{−4}$

* ***Evaluation***
    * For automatically evaluate the quality -> train ResNeXT classifier on SC09
    * Fréchet Inception Distance (FID) (Heusel et al., 2017) 
        * Measures both quality and diversity of generated samples
        * Favors generators that match moments in the feature space. 
    * Inception Score (IS) (Salimans et al., 2016)
        * Measures both quality and diversity of generated samples
        * Favors generated samples that can be clearly determined by the classifier.
    * Modified Inception Score (mIS) (Gurumurthy et al., 2017)
        * Measures the within-class diversity of samples in addition to IS.
    * AM Score (Zhou et al., 2017)
        * Takes into consideration the marginal label distribution of training data compared to IS.
    * Number of Statistically-Different Bins (NDB) (Richardson & Weiss, 2018)
        * Measures diversity of generated samples.

* ***Results***

### 3.2. Class-Conditional Generation
* Digit label을 DiffWave 제공.
* WaveGAN은 noise가 너무 많아서 실험에서 제외
* Unconditional exp.와 setting은 동일
* ***Evaluation***
    * AMscore & NDB
        * Prior label distribution이 주어지는 경우 그렇게 의미있는 measure가 아니기 때문에 제외
    * IS & mIS
        * Keep IS and mIS, because IS favors sharp, clear samples and mIS measures within-class diversity
    * FID to FID-class
        * Compute FID between the generated audio samples that are pre-specified as this digit and training utterances with the same digit labels
        * And report the mean and standard deviation of these ten FID scores. 
    * Accuracy 
        * Based on the ResNeXT classifier used in Section 5.2.

* ***Results***
    * Deep and thin version of DiffWave with residual channels C = 128 and 48 residual layers can achieve slightly better accuracy but lower audio quality.
    * 전체적으로 label을 줄때가 label을 주지 않은 unconditional exp보다 좋은 MOS 와 IS score 가짐
        * Digit label이 generative task의 난이도를 낮추고, generation quality 높이데에 역할을 함

### 3.4. ADDITIONAL RESULTS
* ***Zero-shot speech denoising***
    * Noise type: white noise, pink noise, running tap, exercise bike, dude miaowing, and doing the dishes
        * Speech Command dataset에 포함되어 있음.
        * Unconditional task training에 사용되지 않음.
    * We add 10% of each type of noise to test data (?), feed these noisy utterances into the reverse process at t = 25, and then obtain the outputs x_0’s. 
    * Model이 denoising task에 대해서 학습된 적도 없고, noise type에 대해서 diffusion 과정에서 추가되는 white noise를 제외하고는 zero knowledge를 가진 모델이지만 좋은 퀄리티를 보임
        * It indicates DiffWave learns a good prior of raw audio.
    * ***Opinion: 다양한 noise 라고 하지만.. 사실 다 white noise 처럼 들림..***

* ***Interpolation in latent space***
    * We can do interpolation with the digit conditioned DiffWave model in Section 5.3 on the SC09 dataset. 
    * The interpolation of voices $x^a_0$, $x^b_0$ between two speakers $a$, $b$ is done in the latent space at $t = 50$
    * We first sample $x^a_t ∼ q(x_t \mid x^a_0)$ and $x^b_t ∼ q(x_t \mid x^b_0)$ for the two speakers. 
    * We then do linear interpolation between $x^a_t$ and $x^b_t$:\
    <br>
    <span align="center">$x^\lambda_t = (1-\lambda)x^a_t + \lambda x^b_t,     \text{for}  0<\lambda<1$</span>
    <br>
    <!-- <span style="display: inline-block; width: 95%; text-align: right;"> -->


    * Finally, we sample $x^\lambda_0 ∼ p_\theta(x^\lambda_0 \mid x^\lambda_t)$. The audio samples are in Section VI on the demo website.
    * ***Opinion: 생각보다 interpolation이 가까운 부분에서만 수행되는 것으로 보임..***

### 4. Conclusion
* ***Future Work***
    * Generate longer utterances, as DiffWave potentially has very large receptive fields
    * Optimizing the inference speed
        * Most effective denoising steps in the reverse process occur near x0, which suggests an even smaller T is possible in DiffWave. 
        * The model parameters θ are shared across the reverse process, so the persistent kernels that stash the parameters on-chip would largely speed-up inference on GPUs (Diamos et al., 2016). 

<!--     
#### Opinion
Wow… very difficult… Diffusion 모델에 대한 이해가 더 필요할듯.
Diffusion 과 flow의 차이는??
Point
diffusion 모델의 첫 적용
다양한 실험
conditional & unconditional generation 둘다 가능한 모델
unconditional을 잘한다 -> “audio 특성에 대해 잘 이해한다” 라고 해석했음.
실제 활용할 수  있냐.. 실험을 해볼 가치가 있냐?
음… 좀 더 지켜보는게 좋을 수 있을 것 같다. -->

