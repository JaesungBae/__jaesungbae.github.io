---
layout: single
title:  "[PR] DiffWave: A Versatile Diffusion Model For Audio Synthesis"
date:   2021-05-18 21:14:06 +0900
categories: paper-review icassp2021 tts:model
use_math: true
toc: true
---

*Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, Bryan Catanzaro* \
Computer Science and Engineering, UCSD & NVIDIA & Baidu Research \
Arxiv: [https://arxiv.org/pdf/2009.09761.pdf](https://arxiv.org/pdf/2009.09761.pdf)

## Abstract
* DiffWave: versatile (다재다능한) diffusion probabilistic model for conditional and unconditional waveform generation.
    1) non-autoregressive
    2) converts the white noise signal into structured waveform 
        * through a Markov chain 
        * with a constant number of steps at synthesis.
    * It is efficiently trained by optimizing a variant of variational bound on the data likelihood.
    * generates high-fidelity audio
        * neural vocoding conditioned on mel spectrogram
        * class-conditional generation
        * **unconditional generation**
    * 특히, challenging한 unconditional generation task 에서 autoregressive와 GAN-based waveform 모델보다 성능이 뛰어남
        * Audio quality & sample diversity

## 1. Introduction
* 이전의 waveform generation 모델
    * Likelihood-based models
        * autoregressive & flow-based models
        * audio synthesis에서 지배적인 모델
        * $\because$ simple training objective & superior ability of modeling the fine details of waveform in real data
    * Training을 위해 auxiliary loss를 필요로 하는 모델
        * flow-based models trained by distillation
        * variational auto-encoder (VAE) based model
        * generative adversarial network (GAN) based models
    * **<span style="color:red">한계</span>**: 이전의 waveform generation 모델은 강력한 local condition (mel or linguistic features)을 가지는 케이스에 집중하고 있음.
        * **<span style="color:red">한계</span>**: Autoregressive model은 unconditional한 setting에서 made-up word-like sounds 또는 inferior (열등한) samples을 만든다고 알려져 있음
        * $\because$ Long sequence (16000 samples per sec)을 conditional 한 정보 없이 생성해야 하기 때문
\\
* Diffusion probabilistic models (= diffusion model)
    * Markov chain을 이용해서 simple distribution (e.g.,  isotropic Gaussian)을 복잡한 distribution으로 바꿔주는 generative model
    * Efficiently trained by optimizing the variational lower bound (ELBO). 
    * can use a diffusion (noise-adding) process without learnable parameters to obtain the “whitened” latents from training data.
        * Therefore, training 과정에서 추가적인 neural network 필요 없음. 
            * 다른 모델과는 반대되는 특성 (e.g., the encoder in VAE or the discriminator in GAN)
        * **<span style="color:blue">장점:</span>** 두가지 network를 joint training 함으로써 발생하는 challenging한 “posterior collapse” or “mode collapse”같은 문제가 발생하지 않음.

DiffWave \\
* raw audio synthesis
* Advantages
    1. non-autoregressive -> synthesis high dim. waveform in parallel
    2. flexible
        * flow-based 모델과 달리 어떠한 architectural 제한도 두지 않음.
        * flow-based model은 latent와 data 간에 bijection (일대일 대응) 해야한다는 제한이 있음. 
        * small-footprint neural network가 되도록 함 -> 그래도 성능이 좋긴함.
    3. single ELBO-based training objective w/o any auxiliary losses (spectrogram reconstruction)
    4. versatile model: condition & uncondition case 모두에서 좋은 성능의 audio 생성

Contribution
1. DiffWave는 feed-forward & bidirectional dilated conv. architecture를 가짐. 
    * WaveNet으로부터motivated
    * WaveNet 수준의 speech quality
    * WaveNet보다 훨씬 빠른 생성 속도
        * few sequential steps (6) 만이 필요로 되기 때문에
2. Small DiffWave 모델은 2.64M parameters를 가지고, synthesizes 22.05 kHz high-fidelity speech (MOS: 4.37) more than 5× faster than real-time on a V100 GPU without engineered kernels
    * SOTA flow-based model 보다는 느린 속도, but 더 작은 footprint
3. Unconditional & class-conditional waveform generation tasks에서 WaveGAN과 WaveNet에 비해 월등히 좋은 성능 얻어냄


# INPROGRESS...
## 2. Method
![f2](/images/2021-05-11-PR_Adaspeech2/f2.PNG){: width="50%" height="50%" .center}

* Two main modules:
    1. TTS Model pipeline: phoneme encoder + mel-decoder
    2. Mel-Encoder
* Steps
    1. Source model training
    2. Mel-encoder aligning
        * Alignment loss를 사용해서 mel encoder의 output space가 phoneme encoder와 가까워지도록 함.
    3. Untranscribed speech adaptation
        * Mel-decoder가 mel-encoder의 도움을 받아 target speaker에 대해서 adaptation 됨.
    4. Inference
        * phoneme-encoder와 mel-decoder 이용해서 음성 생성

### 2.1. Source Model Training
![f1](/images/2021-05-11-PR_Adaspeech2/f1.PNG){: width="50%" height="50%" .center}

* AdaSpeech 구조 따름
    * specifically designed acoustic condition modeling + conditional layer normalization
    * Phoneme-encoder
        * 4개의 FFT block
        * GT duration 사용해서 upsampling
        * GT duration extraction에 MFA 사용됨
    * Mel-decoder
        * 4개의 FFT Block
        * Adds more variance information including pitch & more phoneme-level acoustic condition info. (AdaSpeech)

### 2.2. Mel-spectrogram encoder aligning
* Mel-decoder의 입장에서 phoneme encoder와 mel-encoder의 output들이 동일한 space에 존재할 것이라고 기대할 것임.
* 그래야 untranscribed speech data로 adaptation한 것이 inference시에 smooth하게 switching이 가능할 것임.
* TTS Model 잘 training 한 뒤에 source transcribed speech dataset 이용하여 eml-encoder가 phoneme-encoder와 가까워지도록 학습함. (L2 loss)
    * Source TTS model (phoneme encoder, mel decoder)는 freeze해서 학습시킴.
    * 이를 통해서 pluggable한 모델이 될 수 있음.

### 2.3. Untranscribed speech adaptation
* Only adapt the parameters related to the conditional layer normalization (AdaSpeech)
    * small amount of parameter만 adapt 해도 됨.

### 2.4. Inference
pass

## 3. Experiments and results
### 3.1. Datasets and Experimental Setup
* Data
    * Source: Libri TTS (586 h & 2456 speakers)
    * Adapt: VCTK (44 h & 1 speaker), LJSpeech (single & 24 h)
    * 실제 발화 상황과 유사한 internal dataset
* Preprocessing
    * 16KHz down sampling
    * 12.5 ms hop size & 50 ms window size
    * G2p를 통한 phoneme 변환
* Setup
    * Hidden dimension = 256 (including the embedding size, the hidden in self-attention, and the input and output of feed-forward network)
    * Attention head = 2 & feed-forward filter size = 1024 & kernel size = 9
* Training
    * 100k steps to optimize the TTS pipeline
    * 10k step to train mel-encoder
    * 2k step for adaptation (mel-decoder)
    * Adam optimizer is used with β1 = 0.9, β2 = 0.98,  = 10−9

### 3.2.  The Quality of Adaptation Voice
* 비교모델
    1. GT
    1. GT mel + Vocoder (MelGAN)
    1. a joint training method
        * Trains the mel-encoder and the phoneme encoder at the same time
            * Similar to some previous adaptable TTS systems using untranscribed data [12]. 
        * 제안한 orderly training strategy가 효과적으로 mel-encoder가 encoder의 output space로 부터 멀어지는 것을 방지하여주고, 성능도 높아지는 것을 증명하기 위한 ***baseline 모델***로 사용.
    1. a PPG-based method
        * Mel-encoder 대신에 PPG-encoder 사용함. (structure는 같음, input만 다름)
        * Uses PPGs (phonetic posteriorgrams) [22, 23] extracted from the untranscribed speech to fine-tune the TTS model. 
            * 내부 ASR 모델을 이용하여 512차원의 PPG extraction  (mel과 길이는 같음)
            * Dense layer 이용해서 256 dim으로 줄인다음 PPG encoder의 input으로 사용됨.
        * PPG-based method를 ***upper bound***로 설정함. 
            * 추가적인 ASR을 이용하여 text/phoneme과 유사한 정보인 PPG를 뽑아내기 때문에
    1. AdaSpeech [8].
        * Previous TTS adaptation system using **paired text and speech (transcribed speech) data.**
        * We take its performance as another ***upper bound.***

![t1](/images/2021-05-11-PR_Adaspeech2/t1.PNG){: width="50%" height="50%" .center}
* Evaluation (MOS & SMOS)
    * Twenty native English speakers are asked to make quality judgments in terms of naturalness and similarity
* 결과
    1. GT recording & 2개의 upper bound 모델들 (AdaSpeech & the PPG-based method)에 동등한 성능 보임. Joint-training 보다는 월등히 뛰어난 성능 보임.
    2. SMOS에 대해서는 upper bound model 보다 약간 안좋은 (0.1점) 성능 보임. 그러나 여전히 Joint-training 보다는 월등히 좋음
    3. Transcribed가 있는 경우 (AdaSpeech) 와 없는 경우 (AdaSpeech2)에 대해서 CMOS 비교도 수행함. 
        * Internal spontaneous speech data에 대해서 실험 수행
        * 비슷한 성능 보임 (AdaSpeech가 0.012 CMOS socre 보임)

### <span style="color:red">3.3. Analyses on Adaptation Strategy (2가지 실험)</span>
![t2](/images/2021-05-11-PR_Adaspeech2/t2.PNG){: width="50%" height="50%" .center}
* 실험 1. W/o L2 loss constraint
    * L2 loss를 통해서 mel-encoder phoneme encoder 가깝게 하는 것의 효과를 관찰하기 위한 ablation study.
* 실험 2. Fine-tune mel encoder & decoder
    * Mel-decoder 뿐만아니라 mel-encoder도 함께 fine-tuning 했을 때의 성능 관찰
    * Voice Qaulity 저하
    * (그러므로) encoder를 변하지 않도록 하는것이 adaptation performance에 긍정적인 영향을 미친다

### <span style="color:red">3.4. Varying Adaptation Data</span>
![f3](/images/2021-05-11-PR_Adaspeech2/f3.PNG){: width="30%" height="30%" .center}

* 샘플 개수에 따른 CMOS evaluation
* 20개 이하일 때 sample 개수가 증가할수록 MOS 점수가 급격하게 증가하는 것을 관찰할 수 있음
* 그러나 이후에는 그렇게 증가가 크지 않음을 관찰할 수 있음

## 4. Conclusion and Comments
* AdaSpeech2
    * pluggable & effective
    * **<span style="color:red">Untranscribed speech data로 adaptation 수행하는 TTS 모델</span>**
    * Upper bound system과 유사한 성능의 MOS와 SMOS 점수 획득
* Future work 
    * We will explore different adaptation methods to improve voice quality and similarity and further extend our method to more challenging scenarios such as spontaneous speech.

***Comments***
* Untranscribed speech dataset을 이용하여 speaker adaptation 수행할 수 있다는 점에서 task 선택 good
* 그리고 뭔가 아이디어 자체가 엄청 특별하지 않지만 신선함. good










<!-- **<span style="color:red">하나의 blank symbol이 모든 2개의 token사이의 transition을 표현할 수 있는가?</span>** -->
<!-- ![1](/images/2021-05-04-PR_talknet2/1.PNG){: .center} -->