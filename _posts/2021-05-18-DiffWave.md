---
layout: single
title:  "[PR] DiffWave: A Versatile Diffusion Model For Audio Synthesis"
date:   2021-05-18 21:14:06 +0900
categories: paper-review icassp2021 tts:model
use_math: true
toc: true
---

*Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, Bryan Catanzaro* \
Computer Science and Engineering, UCSD & NVIDIA & Baidu Research \
Arxiv: [https://arxiv.org/pdf/2009.09761.pdf](https://arxiv.org/pdf/2009.09761.pdf)

## Abstract
* DiffWave: versatile (다재다능한) diffusion probabilistic model for conditional and unconditional waveform generation.
    * 1) non-autoregressive
    * 2) converts the white noise signal into structured waveform 
        * through a Markov chain 
        * with a constant number of steps at synthesis.
    * It is efficiently trained by optimizing a variant of variational bound on the data likelihood.
    * generates high-fidelity audio
        * neural vocoding conditioned on mel spectrogram
        * class-conditional generation
        * **unconditional generation**
    * 특히, challenging한 unconditional generation task 에서 autoregressive와 GAN-based waveform 모델보다 성능이 뛰어남
        * Audio quality & sample diversity

## 1. Introduction
* 이전의 waveform generation 모델
    * Likelihood-based models
        * autoregressive & flow-based models
        * audio synthesis에서 지배적인 모델
        * $\because$ simple training objective & superior ability of modeling the fine details of waveform in real data
    * Training을 위해 auxiliary loss를 필요로 하는 모델
        * flow-based models trained by distillation
        * variational auto-encoder (VAE) based model
        * generative adversarial network (GAN) based models
    * **<span style="color:red">한계</span>**: 이전의 waveform generation 모델은 강력한 local condition (mel or linguistic features)을 가지는 케이스에 집중하고 있음.
        * **<span style="color:red">한계</span>**: Autoregressive model은 unconditional한 setting에서 made-up word-like sounds 또는 inferior (열등한) samples을 만든다고 알려져 있음
        * $\because$ Long sequence (16000 samples per sec)을 conditional 한 정보 없이 생성해야 하기 때문  
<br>

* Diffusion probabilistic models (= diffusion model)
    * Markov chain을 이용해서 simple distribution (e.g.,  isotropic Gaussian)을 복잡한 distribution으로 바꿔주는 generative model
    * Efficiently trained by optimizing the variational lower bound (ELBO). 
    * can use a diffusion (noise-adding) process without learnable parameters to obtain the “whitened” latents from training data.
        * Therefore, training 과정에서 추가적인 neural network 필요 없음. 
            * 다른 모델과는 반대되는 특성 (e.g., the encoder in VAE or the discriminator in GAN)
        * **<span style="color:blue">장점:</span>** 두가지 network를 joint training 함으로써 발생하는 challenging한 “posterior collapse” or “mode collapse”같은 문제가 발생하지 않음.
<br>

* DiffWave
    * Raw audio synthesis
    * Advantages
        1. non-autoregressive -> synthesis high dim. waveform in parallel
        2. flexible
            * flow-based 모델과 달리 어떠한 architectural 제한도 두지 않음.
            * flow-based model은 latent와 data 간에 bijection (일대일 대응) 해야한다는 제한이 있음. 
            * small-footprint neural network가 되도록 함 -> 그래도 성능이 좋긴함.
        3. single ELBO-based training objective w/o any auxiliary losses (spectrogram reconstruction)
        4. versatile model: condition & uncondition case 모두에서 좋은 성능의 audio 생성
<br>

* Contribution
    1. DiffWave는 feed-forward & bidirectional dilated conv. architecture를 가짐. 
        * WaveNet으로부터motivated
        * WaveNet 수준의 speech quality
        * WaveNet보다 훨씬 빠른 생성 속도
            * few sequential steps (6) 만이 필요로 되기 때문에
    2. Small DiffWave 모델은 2.64M parameters를 가지고, synthesizes 22.05 kHz high-fidelity speech (MOS: 4.37) more than 5× faster than real-time on a V100 GPU without engineered kernels
        * SOTA flow-based model 보다는 느린 속도, but 더 작은 footprint
    3. Unconditional & class-conditional waveform generation tasks에서 WaveGAN과 WaveNet에 비해 월등히 좋은 성능 얻어냄


# INPROGRESS...