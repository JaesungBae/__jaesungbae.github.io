---
layout: single
title:  "TalkNet 2: Non-Autoregressive Depth-Wise Separable Convolutional Model for Speech Synthesis with Explicit Pitch and Duration Prediction"
date:   2021-04-27 21:14:06 +0900
categories: paper-review icassp2021 tts:model
---

*Stanislav Beliaev, Boris Ginsburg* \
NVIDIA, Santa Clara \
Arxiv: [https://arxiv.org/pdf/2104.08189.pdf](https://arxiv.org/pdf/2104.08189.pdf)

### Abstract
* Phoneme이 아니라 character에 대해서 수행한줄 알았는데 phoneme에 대해서 수행.
* Character-level duration extraction을 위해서 external ASR **CTC based model** 사용.
    * Greedy output이 아니라 most probable path in the lattice using Viterbi algorithm. 
    * greedy decoding, beam search.
    * Align TTS?
* 1D depth-wise separable **convolutions** -> 13.2M parameters.
* 3가지 모듈이 각각 따로 역할함.
    1. grapheme duration predictor
    2. pitch predictor
    3. mel-spectrogram generator
    * **Text encoder가 없다**

![1](/images/2021-05-04-PR_talknet2/1.PNG){: width="50%" height="50%"}

### Related Work
* Deep Voice: phoneme duration predictor를 학습시키기 위해 auxiliary CTC-based model for phonetic segmentation was used to annotate data with phoneme boundaries.
* FastSpeech의 단점.
    1. Teacher model 필요하다 -> 최근엔 필요 없음
    2. **phoneme duration을 뽑는 teacher model (TTS) 이 inaccurate하다 -> 동의**

### TalkNet
![1](/images/2021-05-04-PR_talknet2/2.PNG)
* Grapheme duration predictor
    * grapheme input **include punctuation.**
    1. **insert blank symbol ~ between every 2 input tokens.**
    2. it predicts the duration for each input token including blank.
    3. expand
    * L2 loss & logarithmic targets.
    * cross-entropy도 시도해 봤는데, 아주 약간이지만 L2 loss의 점수가 더 높았음.
    * **<span style="color:red"> Hidden representation에 대해서 예측 X </span>**

![1](/images/2021-05-04-PR_talknet2/5.PNG)
* Pitch predictor
    * 여기는 VUV를 pitch에 적용해 줬음!
        * FastPitch에도 시도해 본 바 있음.
    * Following [24], we use Gaussian embedding layer for tokens with normal distribution proportional to the token duration. (Gaussian upsampling 말하는 것 같은데..? 는 아닌 것 같은데...)
    * 예측은 fame-level pitch
    * Hidden representation에 대해서 예측 X
* Mel spectrogram generator
    * We also use the Gaussian embedding for expanded input sequence as in pitch predictor. 

### Training
***1. Dataset***
* LJSpeech dataset 이용.

***2. Ground truth duration extraction***
* We extracted the ground truth alignment from a CTC ASR model, similar to [7]. (Deep Voice 논문)
* The blank symbol acts as an intermediate state between two neighbouring graphemes, and its duration **<span style="color:red">roughly corresponds to the length of the transition from one token to another.</span>**
    * **<span style="color:red">하나의 blank symbol이 모든 2개의 token사이의 transition을 표현할 수 있는가?</span>**
* Train ASR model with text (g2p로 변환된 phoneme) with punctuation.
    * Libiri TTS dataset, PER: 8.5% on Libri-TTS clean testset.
* CTC 결과를 그대로 사용하지 않고 log-prob.결과에 대해 Viterbi alignment algorithm 적용.

***3. Grapheme duration and pitch predictors training***
* Adam with $\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=10e^{−8}$ , a weight decay of 10−6 and gradient norm clipping of 1.0. 
* We used a cosine decay learning rate policy starting from 10−3 and decreasing to 10−5 with a 2% warmup.
* batch size: 256
* duration predictor (~200 epoch): 1.3 h
* The duration predictor 성능
    * Accuracy is around 81%, but it covers approximately 95% of classes within an absolute distance of 1. 

***4. Mel-spectrogram generator training***
* batch size 64
* 200 epch = ~8 h
* **<span style="color:red">Point: 각각 training 할 수 있다!</span>**

### Results
![1](/images/2021-05-04-PR_talknet2/6.PNG)
![1](/images/2021-05-04-PR_talknet2/7.PNG)

